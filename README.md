# ğŸ§  RAG-Powered SQL Validation & Execution Platform

This project is a **Retrieval-Augmented Generation (RAG)** based system designed to automate **SQL validation**, **ETL script checking**, and **natural language to SQL generation**. It enables users to upload schemas and transformation scripts, generate embeddings, ask SQL-related questions, validate ETL processes, and even run and push the resulting SQL scripts to GitHub.

---

## ğŸš€ Features

- ğŸ” Semantic chunking and embedding of source/target DB schema and transformation logic.
- ğŸ§  Natural language to SQL generation using LLMs (via Ollama models like `codellama`, `deepseek`, `mistral`).
- âœ… ETL validation suite generation.
- ğŸ—‚ï¸ Embedded vector database powered by FAISS.
- ğŸ’¾ Script saving and GitHub PR generation.
- âš™ï¸ Executable SQL script runner with MySQL integration.
- ğŸ“„ Web interface using Flask.

---

## ğŸ“ Project Structure

| File / Folder          | Description |
|------------------------|-------------|
| `app.py`               | Flask backend for query interface, API endpoints |
| `main.py`              | CLI version to generate embeddings and test queries |
| `new_codebase_rag.py`  | Core RAG logic for embedding, querying, and validation |
| `chunking.py`          | Smart chunking engine for SQL, Python, configs, etc. |
| `connect_alchemy.py`   | MySQL database connection and document preparation |
| `execute_output.py`    | SQL script execution and result capture |
| `new_prompt.py`        | Prompt templates for SQL and ETL validation |
| `rag_config.py`        | FAISS vector DB detection/initialization |
| `config.ini`           | Configuration for database and GitHub |
| `results/`             | Output SQL scripts generated by the system |
| `uploads/`             | Uploaded schema and SQL files for processing |

---

## ğŸ› ï¸ Installation

### 1. Clone the repository

git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
2. Set up Python environment
bash
Copy
Edit
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
Make sure the following packages are included:

PACKAGES:

Copy
Edit
flask
flask_cors
langchain
langchain-community
langchain-experimental
faiss-cpu
sqlparse
pymysql
ollama


3. Start Ollama models
Ensure ollama is installed and start relevant models:

bash
Copy
Edit
ollama run codellama:7b
ollama run deepseek-r1:8b
ollama run mistral:7b
4. Configure your system
Edit config.ini:

ini
Copy
Edit
[DATABASE]
HOST = localhost
USER = root
PASSWORD = password
PORT = 3306
SOURCE_DATABASE = source_db
TARGET_DATABASE = target_db

[GITHUB]
TOKEN = your_github_pat_here
âš™ï¸ How It Works
Step 1: Upload Files
Upload SQL schemas, transformation scripts via the /api/create-embeddings endpoint or UI.

Embeddings will be created and stored in a FAISS vector DB.

Step 2: Query with Natural Language
Ask questions like:

"Validate that the orders table has the same number of rows in source and target"
"Generate SQL to detect NULLs in customer email"

The system:

Retrieves relevant chunks (schemas, transformation logic)

Selects the best LLM based on query type

Uses a tailored prompt to generate SQL

Step 3: Save or Execute Query
Save the SQL query to a .sql file.

Push it to GitHub via a PR.

Optionally execute the query against the configured MySQL DB and get results.

ğŸŒ Running the App
Flask Web App
bash
Copy
Edit
python app.py
Navigate to http://localhost:5000 to access the UI.

ğŸ“¦ API Endpoints
Endpoint	Method	Description
/api/create-embeddings	POST	Upload and embed files
/api/query	POST	Query the RAG system
/api/save-script	POST	Save and optionally push script to GitHub
/api/execute-script	POST	Execute saved SQL script
/api/download-script/<filename>	GET	Download generated SQL

âœ¨ Example Use Case
Upload:

Source/target schema SQL

Transformation script

Query:

"Validate if the revenue column transformation is correctly applied"

Output:

Automatically generated SQL validation queries

Optionally executed and pushed to GitHub

ğŸ§© Future Improvements
LLM-based query classification between analysis, cleaning, and validation

Frontend enhancements for query history and version tracking

Schema diffing between source and target

ğŸ‘¨â€ğŸ’» Author
Built by Tejojith Ganesh Kumar